{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARTIFICIAL NEURAL NETWORK \n",
    "### NOTE: USED ON GOOGLE COLAB, NOT LOCAL DRIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "torch.manual_seed(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(filepath, right_arm=False):\n",
    "    data0 = pd.read_csv(filepath + \".csv\")\n",
    "    data1 = pd.read_csv(filepath + \"1.csv\")\n",
    "    data2 = pd.read_csv(filepath + \"2.csv\")\n",
    "    df = pd.concat([data0, data1, data2], ignore_index=True)\n",
    "    X = \"R\" if right_arm else \"L\"\n",
    "    angle_names = [\"{}ShoulderPitch\".format(X),\n",
    "                   \"{}ShoulderRoll\".format(X),\n",
    "                   \"{}ElbowYaw\".format(X),\n",
    "                   \"{}ElbowRoll\".format(X)]\n",
    "    labels = df[angle_names]\n",
    "    data = df[[column for column in df.columns if column not in angle_names]]\n",
    "    # filter corrupted data!!!\n",
    "    data = data.drop_duplicates(keep=False)\n",
    "    labels = labels.loc[data.index]\n",
    "\n",
    "    data, labels = torch.Tensor(data.values), torch.Tensor(labels.values)\n",
    "    return Data.TensorDataset(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(net_version=None):\n",
    "    if net_version == 0:\n",
    "        net = nn.Sequential(\n",
    "                    nn.Linear(6, 200),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(200, 100),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(100, 100),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(100, 50),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(50, 4)\n",
    "                )\n",
    "    else:\n",
    "        net = nn.Sequential(\n",
    "                    nn.Linear(6, 200),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(200, 100),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(100, 4)\n",
    "                )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ann(filepath, net=None, batch_size=10000, epoch=300, learning_rate=0.01, decay=0, net_version=0):\n",
    "\n",
    "    dataset = get_dataset(filepath)\n",
    "    loader = Data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    if net is None:\n",
    "        net = get_net(net_version)\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "        print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    iters, losses = [], []\n",
    "    n = 0\n",
    "    best_result = np.inf\n",
    "    best_epoch = 0\n",
    "    for epoch in range(epoch):\n",
    "        error = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "        for step, (data, labels) in enumerate(loader):\n",
    "            #############################################\n",
    "            #To Enable GPU Usage\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                labels = labels.cuda()\n",
    "            #############################################\n",
    "            prediction = net(data)\n",
    "            loss = loss_func(prediction, labels) \n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            \n",
    "            iters.append(n)\n",
    "            losses.append(float(loss)/batch_size)             # compute *average* loss\n",
    "            n += 1\n",
    "\n",
    "            for x in range(4):\n",
    "                if torch.cuda.is_available():\n",
    "                    error[x] += mean_absolute_error(prediction[:, x].cpu().detach().numpy(), labels[:,x].cpu().detach().numpy())\n",
    "                else:\n",
    "                    error[x] += mean_absolute_error(prediction[:, x].detach().numpy(), labels[:,x].detach().numpy())\n",
    "        print(\"Epoch: {}, Error: {}\".format(epoch, error / step))\n",
    "        \n",
    "        if sum(error) < best_result:\n",
    "            best_result = sum(error)\n",
    "            best_epoch = epoch\n",
    "            print(\"Best result at Epoch {}. Saving model parameters.\".format(epoch))\n",
    "            model_path = \"ann_epoch{}\".format(epoch)\n",
    "            torch.save(net.state_dict(), model_path)   \n",
    "            \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            # saving the best model at my local google drive for every 100th epoch\n",
    "            model_path = \"ann_epoch{}\".format(best_epoch)\n",
    "            state = torch.load(model_path)\n",
    "            net.load_state_dict(state)\n",
    "            model_path = \"/content/drive/My Drive/Thesis/left_arm_ANN_model\"\n",
    "            torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iters, losses, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_arm_filepath = \"/content/drive/My Drive/Thesis/left_arm_data\"\n",
    "net = get_net()\n",
    "train_ann(left_arm_filepath, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning result\n",
    "train_ann(batch_size=10000, learning_rate=0.01, net_version=0) \n",
    "\n",
    "Epoch: 51, Error: [0.09198165 0.03829048 0.1201902  0.04457075]\n",
    "\n",
    "train_ann(batch_size=10000, learning_rate=0.01, net_version=1) \n",
    "\n",
    "Epoch: 198, Error: [0.0830939  0.01547376 0.11576251 0.02581779]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGH WORK PAST THIS POINT!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, right_arm):\n",
    "    X = \"R\" if right_arm else \"L\"\n",
    "    angle_names = [\"{}ShoulderPitch\".format(X),\n",
    "                   \"{}ShoulderRoll\".format(X),\n",
    "                   \"{}ElbowYaw\".format(X),\n",
    "                   \"{}ElbowRoll\".format(X)]\n",
    "    labels = df[angle_names]\n",
    "    data = df[[column for column in df.columns if column not in angle_names]]\n",
    "    \n",
    "    # filter corrupted data!!!\n",
    "    data = data.drop_duplicates(keep=False)\n",
    "    labels = labels.loc[data.index]\n",
    "    \n",
    "    return train_test_split(data, labels, test_size=0.9, random_state=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(filepath, right_arm=False):\n",
    "    # get data\n",
    "    data0 = pd.read_csv(filepath + \".csv\")\n",
    "    data1 = pd.read_csv(filepath + \"1.csv\")\n",
    "    data2 = pd.read_csv(filepath + \"2.csv\")\n",
    "    df = pd.concat([data0, data1, data2], ignore_index=True)\n",
    "    \n",
    "    # split data\n",
    "    x_train, x_test, y_train, y_test = split_data(df, right_arm)\n",
    "    \n",
    "    # train model\n",
    "    model=RandomForestRegressor(n_estimators=50) # n_estimator = 100 -> 10 GB pickle file\n",
    "                                                 # n_estimator = 50 -> 5 GB pickle file (~0.02 error difference)\n",
    "    model.fit(x_train,y_train)\n",
    "    # get model accuracy\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred = pd.DataFrame(y_pred, columns=y_test.columns)\n",
    "    for column in y_pred.columns:\n",
    "        print(column, \"Error:\",mean_absolute_error(y_test[column], y_pred[column]))\n",
    "    \n",
    "    # save model\n",
    "    filename = 'right_arm_model.pkl' if right_arm else 'left_arm_model.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LShoulderPitch', 'Error:', 0.07815650313318641)\n",
      "('LShoulderRoll', 'Error:', 0.018343106460681394)\n",
      "('LElbowYaw', 'Error:', 0.15007631305212996)\n",
      "('LElbowRoll', 'Error:', 0.0402890007508116)\n"
     ]
    }
   ],
   "source": [
    "left_arm_filepath = \"/home/kevinh/Documents/left_arm_data\"\n",
    "right_arm_filepath = \"/home/kevinh/Documents/right_arm_data\"\n",
    "train(left_arm_filepath)\n",
    "# train(right_arm_filepath, right_arm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = left_arm_filepath\n",
    "data0 = pd.read_csv(filepath + \".csv\")\n",
    "data1 = pd.read_csv(filepath + \"1.csv\")\n",
    "# data2 = pd.read_csv(filepath + \"2.csv\")\n",
    "df = pd.concat([data0, data1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \"L\"\n",
    "angle_names = [\"{}ShoulderPitch\".format(X),\n",
    "               \"{}ShoulderRoll\".format(X),\n",
    "               \"{}ElbowYaw\".format(X),\n",
    "               \"{}ElbowRoll\".format(X)]\n",
    "labels = df[angle_names]\n",
    "data = df[[column for column in df.columns if column not in angle_names]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter corrupted data!!!\n",
    "data = data.drop_duplicates(keep=False)\n",
    "labels = labels.loc[data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.9, random_state=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model=RandomForestRegressor(n_estimators=50)\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LShoulderPitch', 'Error:', 0.11389112294205424)\n",
      "('LShoulderRoll', 'Error:', 0.033337454145367554)\n",
      "('LElbowYaw', 'Error:', 0.2354261375816541)\n",
      "('LElbowRoll', 'Error:', 0.07061861411256729)\n"
     ]
    }
   ],
   "source": [
    "# get model accuracy\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=y_test.columns)\n",
    "for column in y_pred.columns:\n",
    "    print(column, \"Error:\",mean_absolute_error(y_test[column], y_pred[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result (500 thousand data points)\n",
    "*without rounding data*\n",
    "\n",
    "- ('LShoulderPitch', 'Error:', 0.11069172338849657)\n",
    "- ('LShoulderRoll', 'Error:', 0.03154031012584691)\n",
    "- ('LElbowYaw', 'Error:', 0.2300845963747696)\n",
    "- ('LElbowRoll', 'Error:', 0.06800276852256665)\n",
    "\n",
    "*rounding data*\n",
    "\n",
    "- ('LShoulderPitch', 'Error:', 0.11069172338849657)\n",
    "- ('LShoulderRoll', 'Error:', 0.03154031012584691)\n",
    "- ('LElbowYaw', 'Error:', 0.2300845963747696)\n",
    "- ('LElbowRoll', 'Error:', 0.06800276852256665)\n",
    "\n",
    "*n_estimators decreased from 200 to 100*\n",
    "- ('LShoulderPitch', 'Error:', 0.11201086854757766)\n",
    "- ('LShoulderRoll', 'Error:', 0.03214602543515891)\n",
    "- ('LElbowYaw', 'Error:', 0.23213343702121328)\n",
    "- ('LElbowRoll', 'Error:', 0.06910553107129716)\n",
    "\n",
    "*n_estimators decreased from 200 to 50*\n",
    "- ('LShoulderPitch', 'Error:', 0.11389112294205424)\n",
    "- ('LShoulderRoll', 'Error:', 0.033337454145367554)\n",
    "- ('LElbowYaw', 'Error:', 0.2354261375816541)\n",
    "- ('LElbowRoll', 'Error:', 0.07061861411256729)\n",
    "\n",
    "### result (4 million data points)\n",
    "*n_estimator = 50, 5GB pickle file*\n",
    "- ('LShoulderPitch', 'Error:', 0.07815650313318641)\n",
    "- ('LShoulderRoll', 'Error:', 0.018343106460681394)\n",
    "- ('LElbowYaw', 'Error:', 0.15007631305212996)\n",
    "- ('LElbowRoll', 'Error:', 0.0402890007508116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
